{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "    # Get a list of all files in the directory (X_train, y_train, X_test)\n",
    "    file_list = [[file for file in os.listdir(paths) if file.endswith('.tif')] for paths in directory_paths]\n",
    "\n",
    "    # Sort the file list to ensure consistent order\n",
    "    file_list = [sorted(files) for files in file_list]\n",
    "\n",
    "    file_list = [file_list[0][0:4000], file_list[1][0:4000]]\n",
    "\n",
    "    # Initialize an empty array to store the image data\n",
    "    X_train, y_train, X_test = [], [], []\n",
    "\n",
    "    # Iterate through the selected files\n",
    "    for X_train_name, y_train_name in tqdm(zip(file_list[0], file_list[1])):\n",
    "        # Construct the full path to the file\n",
    "        X_train_path = os.path.join(paths[0], X_train_name)\n",
    "        y_train_path = os.path.join(paths[1], y_train_name)\n",
    "\n",
    "        # Open the raster file using rasterio\n",
    "        with rasterio.open(X_train_path) as src:\n",
    "            # Read the entire image data as a NumPy array\n",
    "            image_data = src.read()\n",
    "\n",
    "            # Append the image data to the array\n",
    "            X_train.append(image_data)\n",
    "\n",
    "        # Open the raster file using rasterio\n",
    "        with rasterio.open(y_train_path) as src:\n",
    "            # Read the entire image data as a NumPy array\n",
    "            image_data = src.read()\n",
    "\n",
    "            # Append the image data to the array\n",
    "            y_train.append(image_data)\n",
    "\n",
    "    # Convert the list of arrays to a single NumPy array\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Competitions\\q2-detect-kelp\\venv\\lib\\site-packages\\rasterio\\__init__.py:317: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "4000it [01:35, 41.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the X_train array: (4000, 7, 350, 350)\n",
      "Shape of the y_train array: (4000, 1, 350, 350)\n"
     ]
    }
   ],
   "source": [
    "directory_paths = ['../data/raw/train_satellite', '../data/raw/train_kelp']\n",
    "os.path.abspath(directory_paths[0])\n",
    "X_train, y_train = load_data(directory_paths)\n",
    "\n",
    "print(\"Shape of the X_train array:\", X_train.shape)\n",
    "print(\"Shape of the y_train array:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new binary y_train array of shape (n_images, 1) where 1 indicates kelp and 0 indicates no kelp in the image\n",
    "y_train_binary = np.zeros((y_train.shape[0], 1))\n",
    "for i in range(y_train.shape[0]):\n",
    "    if np.sum(y_train[i]) > 0:\n",
    "        y_train_binary[i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = y_train_binary.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images with kelp: 2495.0\n"
     ]
    }
   ],
   "source": [
    "# Count the number of images with kelp and without kelp\n",
    "print(\"Number of images with kelp:\", np.sum(y_train_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findConv2dOutShape(hin,win,conv,pool=2):\n",
    "    # get conv arguments\n",
    "    kernel_size=conv.kernel_size\n",
    "    stride=conv.stride\n",
    "    padding=conv.padding\n",
    "    dilation=conv.dilation\n",
    "\n",
    "    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
    "    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "\n",
    "    if pool:\n",
    "        hout/=pool\n",
    "        wout/=pool\n",
    "    return int(hout),int(wout)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Neural Network\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    # Network Initialisation\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "    \n",
    "        Cin,Hin,Win=params[\"shape_in\"]\n",
    "        init_f=params[\"initial_filters\"] \n",
    "        num_fc1=params[\"num_fc1\"]  \n",
    "        num_classes=params[\"num_classes\"] \n",
    "        self.dropout_rate=params[\"dropout_rate\"] \n",
    "        \n",
    "        # Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n",
    "        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv2)\n",
    "        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv3)\n",
    "        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv4)\n",
    "        \n",
    "        # compute the flatten size\n",
    "        self.num_flatten=h*w*8*init_f\n",
    "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
    "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
    "\n",
    "    def forward(self,X):\n",
    "        \n",
    "        # Convolution & Pool Layers\n",
    "        X = F.relu(self.conv1(X)); \n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv4(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "\n",
    "        X = X.view(-1, self.num_flatten)\n",
    "        \n",
    "        X = F.relu(self.fc1(X))\n",
    "        X=F.dropout(X, self.dropout_rate)\n",
    "        X = self.fc2(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PyTorch dataset\n",
    "from torch.utils.data import Dataset\n",
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images with kelp in y_train_split: 1999.0\n",
      "Number of images with kelp in y_test_split: 496.0\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, y_train_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Find balance of kelp in y_train_split and y_test_split\n",
    "print(\"Number of images with kelp in y_train_split:\", np.sum(y_train_split))\n",
    "print(\"Number of images with kelp in y_test_split:\", np.sum(y_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6588 Acc: 0.6494\n",
      "Validation Loss: 0.6270 Acc: 0.6875\n",
      "Training Loss: 0.6116 Acc: 0.6809\n",
      "Validation Loss: 0.5938 Acc: 0.6987\n",
      "Training Loss: 0.6030 Acc: 0.6878\n",
      "Validation Loss: 0.5986 Acc: 0.6887\n",
      "Training Loss: 0.5973 Acc: 0.6959\n",
      "Validation Loss: 0.5882 Acc: 0.7000\n",
      "Training Loss: 0.5865 Acc: 0.7016\n",
      "Validation Loss: 0.5768 Acc: 0.7037\n",
      "Training Loss: 0.5768 Acc: 0.7134\n",
      "Validation Loss: 0.5768 Acc: 0.7087\n",
      "Training Loss: 0.5772 Acc: 0.7034\n",
      "Validation Loss: 0.5826 Acc: 0.6963\n",
      "Training Loss: 0.5626 Acc: 0.7203\n",
      "Validation Loss: 0.5912 Acc: 0.6937\n",
      "Training Loss: 0.5520 Acc: 0.7191\n",
      "Validation Loss: 0.5887 Acc: 0.7025\n",
      "Training Loss: 0.5357 Acc: 0.7234\n",
      "Validation Loss: 0.6088 Acc: 0.7000\n",
      "Training Loss: 0.5653 Acc: 0.7206\n",
      "Validation Loss: 0.6174 Acc: 0.6925\n",
      "Training Loss: 0.5134 Acc: 0.7394\n",
      "Validation Loss: 0.6627 Acc: 0.6850\n",
      "Training Loss: 0.5182 Acc: 0.7356\n",
      "Validation Loss: 0.6215 Acc: 0.7013\n",
      "Training Loss: 0.4717 Acc: 0.7506\n",
      "Validation Loss: 0.6975 Acc: 0.6813\n",
      "Training Loss: 0.4236 Acc: 0.7800\n",
      "Validation Loss: 0.8360 Acc: 0.6763\n",
      "Training Loss: 0.3744 Acc: 0.8213\n",
      "Validation Loss: 0.9240 Acc: 0.7050\n",
      "Training Loss: 0.3275 Acc: 0.8413\n",
      "Validation Loss: 1.0064 Acc: 0.6887\n",
      "Training Loss: 0.2908 Acc: 0.8709\n",
      "Validation Loss: 1.1201 Acc: 0.6775\n",
      "Training Loss: 0.2348 Acc: 0.9019\n",
      "Validation Loss: 1.3238 Acc: 0.6675\n",
      "Training Loss: 0.1815 Acc: 0.9306\n",
      "Validation Loss: 1.3766 Acc: 0.6837\n",
      "Training Loss: 0.1548 Acc: 0.9453\n",
      "Validation Loss: 2.0114 Acc: 0.6863\n",
      "Training Loss: 0.1005 Acc: 0.9650\n",
      "Validation Loss: 1.9923 Acc: 0.6663\n",
      "Training Loss: 0.0820 Acc: 0.9738\n",
      "Validation Loss: 2.1768 Acc: 0.7000\n",
      "Training Loss: 0.0575 Acc: 0.9834\n",
      "Validation Loss: 2.4973 Acc: 0.6625\n",
      "Training Loss: 0.0679 Acc: 0.9772\n",
      "Validation Loss: 2.2196 Acc: 0.6975\n",
      "Training Loss: 0.0792 Acc: 0.9784\n",
      "Validation Loss: 2.4143 Acc: 0.6725\n",
      "Training Loss: 0.0447 Acc: 0.9894\n",
      "Validation Loss: 2.4159 Acc: 0.6813\n",
      "Training Loss: 0.0324 Acc: 0.9909\n",
      "Validation Loss: 2.7961 Acc: 0.6850\n",
      "Training Loss: 0.0232 Acc: 0.9947\n",
      "Validation Loss: 3.0464 Acc: 0.6937\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     65\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Competitions\\q2-detect-kelp\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Competitions\\q2-detect-kelp\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 54\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     52\u001b[0m X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(X, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     53\u001b[0m X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(X))\n\u001b[1;32m---> 54\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(X))\n\u001b[0;32m     56\u001b[0m X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(X, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Competitions\\q2-detect-kelp\\venv\\lib\\site-packages\\torch\\_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Competitions\\q2-detect-kelp\\venv\\lib\\site-packages\\torch\\nn\\functional.py:791\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    790\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the training parameters\n",
    "params_train = {'batch_size': 64,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0}\n",
    "\n",
    "params_val = {'batch_size': 64,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0}\n",
    "\n",
    "# Define the training and validation sets\n",
    "X_train = X_train / 4000\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, y_train_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the training and validation datasets\n",
    "dataset_train = SatelliteDataset(X_train_split, y_train_split)\n",
    "dataset_val = SatelliteDataset(X_test_split, y_test_split)\n",
    "\n",
    "# Create the training and validation generators\n",
    "training_generator = DataLoader(dataset_train, **params_train)\n",
    "validation_generator = DataLoader(dataset_val, **params_val)\n",
    "\n",
    "# Define the model parameters\n",
    "model_params = {\n",
    "    \"shape_in\": (7, 350, 350),\n",
    "    \"initial_filters\": 8,\n",
    "    \"num_fc1\": 100,\n",
    "    \"num_classes\": 2,\n",
    "    \"dropout_rate\": 0.25\n",
    "}\n",
    "\n",
    "# Initialize the neural network\n",
    "model = Network(model_params)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Run the training loop\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in training_generator:\n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate the statistics\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.long().data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset_train)\n",
    "    epoch_acc = running_corrects.double() / len(dataset_train)\n",
    "\n",
    "    print('Training Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in validation_generator:\n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "\n",
    "        # Calculate the statistics\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.long().data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset_val)\n",
    "    epoch_acc = running_corrects.double() / len(dataset_val)\n",
    "\n",
    "    print('Validation Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
