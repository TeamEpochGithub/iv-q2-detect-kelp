defaults:
  - pipeline/default@_here_
  - _self_

feature_pipeline:
  processed_path: data/processed
  transformation_pipeline:
    transformations: []
  column_pipeline:
    columns: []

target_pipeline:

model_loop_pipeline:
  pretrain_pipeline:
    pretrain_steps:
      - _target_: src.pipeline.model.model_loop.pretrain.gbdt.GBDT
        max_images: 500
        early_stopping_split: 0.2
      - _target_: src.pipeline.model.model_loop.pretrain.column_selection.ColumnSelection
        columns: [0, 1, 2, 7]
      - _target_: src.pipeline.model.model_loop.pretrain.scaler_block.ScalerBlock
        scaler:
          _target_: dask_ml.preprocessing.StandardScaler

  model_blocks_pipeline:
    model_blocks:
      # Pytorch model
      - _target_: src.pipeline.model.model_loop.model_blocks.torch_block.TorchBlock
        model:
          _target_: src.pipeline.model.architectures.grid_model.GridModel
          model:
            _target_: segmentation_models_pytorch.Unet
            encoder_name: efficientnet-b0
            in_channels: 4
            classes: 1
            activation: sigmoid

        optimizer: # Partially instantiate optimizer, so model parameters can be linked at runtime
          _target_: functools.partial
          _args_:
            - _target_: hydra.utils.get_class
              path: torch.optim.AdamW
          lr: 0.005

        scheduler:
          _target_: functools.partial
          _args_:
            - _target_: hydra.utils.get_class
              path: torch.optim.lr_scheduler.CosineAnnealingLR
          T_max: 30

        criterion:
          _target_: src.modules.loss.dice_loss.DiceLoss

        epochs: 20
        batch_size: 64
        patience: 5
        transformations:

post_processing_pipeline:
