defaults:
  - pipeline/default@_here_
  - _self_

feature_pipeline:
  processed_path: data/processed
  transformation_pipeline:
  column_pipeline:
    columns:

      # NDVI
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.norm_diff.NormDiff
          a: 1 # NIR
          b: 2 # Red
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

      # NDWI
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.norm_diff.NormDiff
          a: 3 # Green
          b: 1 # NIR
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

      # ONIR
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.offset.Offset
          band: 1 # NIR
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

      # ODVI
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.offset_diff.OffsetDiff
          a: 1 # NIR
          b: 2 # Red
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

target_pipeline:

model_loop_pipeline:
  pretrain_pipeline:
    pretrain_steps:
      - _target_: src.pipeline.model.model_loop.pretrain.column_selection.ColumnSelection
        columns: [1,7,8,9,10]
      - _target_: src.pipeline.model.model_loop.pretrain.scaler_block.ScalerBlock
        scaler:
          _target_: dask_ml.preprocessing.StandardScaler

  model_blocks_pipeline:
    model_blocks:
      # Pytorch model
      - _target_: src.pipeline.model.model_loop.model_blocks.torch_block.TorchBlock
        model:
          _target_: src.pipeline.model.architectures.padded_model.PaddedModel
          padding: 1
          model: # from segmentation_models_pytorch import Unet
            _target_: segmentation_models_pytorch.Unet
            encoder_name: resnet34
            in_channels: 5
            classes: 1
            activation: sigmoid
        optimizer: # Partially instantiate optimizer, so model parameters can be linked at runtime
          _target_: functools.partial
          _args_:
            - _target_: hydra.utils.get_class
              path: torch.optim.Adam
          lr: 0.001
        scheduler: None
        criterion:
          _target_: src.modules.loss.combo_loss.ComboLoss
        epochs: 5
        batch_size: 16
        patience: 10


# Post processing pipeline
post_processing_pipeline:
