defaults:
  - pipeline/default@_here_
  - _self_

feature_pipeline:
  processed_path: data/processed
  transformation_pipeline:
    transformations:

      # Divider
      - _target_: src.pipeline.model.feature.transformation.divider.Divider
        divider: 2

  column_pipeline:
    columns:

      # BandCopy
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.band_copy.BandCopy
          band: 1
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

target_pipeline:

model_loop_pipeline:
  pretrain_pipeline:
    pretrain_steps:
      - _target_: src.pipeline.model.model_loop.pretrain.scaler_block.ScalerBlock
        scaler:
          _target_: dask_ml.preprocessing.StandardScaler

  model_blocks_pipeline:
    model_blocks:

      # Pytorch model
      - _target_: src.pipeline.model.model_loop.model_blocks.torch_block.TorchBlock
        model:
          _target_: src.pipeline.model.architectures.padded_model.PaddedModel
          padding: 1
          model: # from segmentation_models_pytorch import Unet
            _target_: segmentation_models_pytorch.Unet
            encoder_name: efficientnet-b0
            in_channels: 8
            classes: 1
            activation: sigmoid
        optimizer: # Partially instantiate optimizer, so model parameters can be linked at runtime
          _target_: functools.partial
          _args_:
            - _target_: hydra.utils.get_class
              path: torch.optim.Adam
          lr: 0.0001
        scheduler: None
        criterion:
          _target_: src.modules.dice_loss.DiceLoss
        epochs: 1
        batch_size: 64
        patience: 20
        transformations:
          _target_: albumentations.Compose
          transforms:
            - _target_: albumentations.VerticalFlip
              p: 0.2

post_processing_pipeline:
