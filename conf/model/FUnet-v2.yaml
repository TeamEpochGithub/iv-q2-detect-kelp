defaults:
  - pipeline/default@_here_
  - _self_

feature_pipeline:
  processed_path: data/processed
  transformation_pipeline:
  column_pipeline:
    columns:

      # NDVI
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.norm_diff.NormDiff
          a: 1 # NIR
          b: 2 # Red
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

      # NDWI
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.norm_diff.NormDiff
          a: 3 # Green
          b: 1 # NIR
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

      # ONIR
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.offset.Offset
          band: 1 # NIR
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

target_pipeline:


model_loop_pipeline:
  pretrain_pipeline:
    pretrain_steps:
      - _target_: src.pipeline.model.model_loop.pretrain.custom_scaler_block.CustomScalerBlock
        scaler:
          _target_: src.pipeline.model.model_loop.pretrain.utils.custom_scaler.CustomStandardScaler


  model_blocks_pipeline:
    model_blocks:
      # Pytorch model
      - _target_: src.pipeline.model.model_loop.model_blocks.torch_block.TorchBlock
        model:
          _target_: src.pipeline.model.architectures.padded_model.PaddedModel
          padding: 1
          model: # from segmentation_models_pytorch import Unet
            _target_: segmentation_models_pytorch.Unet
            encoder_name: vgg11
            in_channels: 10
            classes: 1
            activation: sigmoid
        optimizer: # Partially instantiate optimizer, so model parameters can be linked at runtime
          _target_: functools.partial
          _args_:
            - _target_: hydra.utils.get_class
              path: torch.optim.Adam
          lr: 0.001
        scheduler:
        criterion:
          _target_: src.modules.loss.dice_loss.DiceLoss
          multiply_kelp: True
        epochs: 100
        batch_size: 16
        patience: 20
        transformations:
          _target_: src.augmentations.transformations.Transformations
          alb:
            _target_: albumentations.Compose
            transforms:
              - _target_: albumentations.VerticalFlip
                p: 0.4
              - _target_: albumentations.HorizontalFlip
                p: 0.4
              - _target_: albumentations.RandomRotate90
                p: 0.4
              - _target_: albumentations.GaussianBlur
                blur_limit: 7
                p: 0.05
          aug:
            - _target_: src.augmentations.mosaic.Mosaic
              p: 0.25

post_processing_pipeline:
  post_processing_steps:
    - _target_: src.pipeline.model.post_processing.visualize_preds_block.VisualizationBlock
      raw_data_path: data/raw/train_satellite
    - _target_: src.pipeline.model.post_processing.threshold.Threshold
