defaults:
  - pipeline/default@_here_
  - _self_

feature_pipeline:
  processed_path: data/processed
  transformation_pipeline:
  column_pipeline:
    columns:

      # NDVI
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.norm_diff.NormDiff
          a: 1 # NIR
          b: 2 # Red
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

      # NDWI
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.norm_diff.NormDiff
          a: 3 # Green
          b: 1 # NIR
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

      # ONIR
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.offset.Offset
          band: 1 # NIR
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

      # ODVI
      - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
        column_block:
          _target_: src.pipeline.model.feature.column.offset_diff.OffsetDiff
          a: 1 # NIR
          b: 2 # Red
        cache_block:
          _target_: src.pipeline.caching.column.CacheColumnBlock
          data_path: data/processed/cache
          column: -1

target_pipeline:

model_loop_pipeline:
  pretrain_pipeline:
    pretrain_steps:
      - _target_: src.pipeline.model.model_loop.pretrain.scaler_block.ScalerBlock
        scaler:
          _target_: dask_ml.preprocessing.StandardScaler

  model_blocks_pipeline:
    model_blocks:

      - _target_: src.pipeline.model.model_loop.model_blocks.torch_block.TorchBlock

        # Pytorch model
        model:
          _target_: src.pipeline.model.architectures.padded_model.PaddedModel
          padding: 1
          model: # from segmentation_models_pytorch import Unet
            _target_: torchvision.models.SwinTransformer
            patch_size: [16, 16]
            embed_dim: 96
            depths: [2, 2, 6, 2]
            num_heads: [11, 6, 12, 24]
            window_size: [7, 3]
        epochs: 30
        batch_size: 16
        patience: 20
        layerwise_lr_decay: 0.9

        # Optimizer
        optimizer:
          _target_: functools.partial
          _args_:
            - _target_: hydra.utils.get_class
              path: torch.optim.AdamW
          lr: 0.00025
          weight_decay: 1e-8

        # Scheduler
        scheduler:
          _target_: functools.partial
          _args_:
            - _target_: hydra.utils.get_class
              path: torch.optim.lr_scheduler.CosineAnnealingLR
          T_max: 30

        criterion:
          _target_: src.modules.loss.dice_loss.DiceLoss

        # Augmentations
        transformations:
          _target_: albumentations.Compose
          transforms:
          - _target_: albumentations.VerticalFlip
            p: 0.5
          - _target_: albumentations.HorizontalFlip
            p: 0.5
          - _target_: albumentations.RandomRotate90
            p: 1
          - _target_: albumentations.GaussianBlur
            blur_limit: 7
            p: 0.2
          - _target_: albumentations.RandomResizedCrop
            height: 350
            width: 350
            scale: [0.8, 1.0]
            ratio: [0.8, 1.2]
            p: 0.5
          - _target_: albumentations.ElasticTransform
            alpha: 1
            sigma: 50
            alpha_affine: 50
            p: 0.5


# Post processing pipeline
post_processing_pipeline:
