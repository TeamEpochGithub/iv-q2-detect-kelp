defaults:
  - pipeline/default
  - _self_

hydra:
  job_logging:
    formatters:
      simple:
        format: "%(asctime)s | %(levelname)s | %(name)s | %(filename)s:%(lineno)d | %(message)s"

raw_data_path: data/raw/train_satellite
raw_target_path: data/raw/train_kelp
split: 0.2

pipeline:
  feature_pipeline:
    processed_path: data/processed

    transformation_pipeline:
      transformations:

        # Divider
        - _target_: src.pipeline.model.feature.transformation.divider.Divider
          divider: 2

    column_pipeline:
      columns:

        # BandCopy
        - _target_: src.pipeline.model.feature.column.column_block.ColumnBlockPipeline
          column_block:
            _target_: src.pipeline.model.feature.column.band_copy.BandCopy
            band: 1
          cache_block:
            _target_: src.pipeline.caching.column.CacheColumnBlock
            data_path: data/processed/cache
            column: -1

  # Targets
  target_pipeline:

  # Model Loop
  model_loop_pipeline:
    pretrain_pipeline:


    model_blocks_pipeline:
      model_blocks:

        # Pytorch model
        - _target_: src.pipeline.model.model_loop.model_blocks.torch_block.TorchBlock
          model:
              _target_: torch.nn.Conv2d
              in_channels: 8
              out_channels: 1
              kernel_size: 3
              padding: 1
          optimizer: # Partially instantiate optimizer, so model parameters can be linked at runtime
              _target_: functools.partial
              _args_:
                  - _target_: hydra.utils.get_class
                    path: torch.optim.Adam
              lr: 0.001

          scheduler: None
          criterion:
            _target_: src.modules.dice_loss.DiceLoss
          epochs: 1
          batch_size: 32
          patience: 10

  # Post processing pipeline
  post_processing_pipeline:
